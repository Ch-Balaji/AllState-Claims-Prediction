{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-dataset/test.csv\n",
      "/kaggle/input/train-dataset/train.csv\n",
      "/kaggle/input/test-orig/test.csv\n",
      "/kaggle/input/submission/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "* we load the data.\n",
    "* Take out categorical and continous feature names.\n",
    "* Factorize the dataset to make model ready.\n",
    "* apply log_shift transformation on target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind data from csv files.....\n",
      "shape of train data : (188318, 132)\n",
      "shape of test data : (125546, 131)\n",
      "Joining complete....\n",
      "shape of combined data: (313864, 132)\n",
      "Total number of categorical features in data are : 116\n",
      "Features that have diff unique values in train and test are :\n",
      "['cat90', 'cat92', 'cat96', 'cat99', 'cat101', 'cat102', 'cat103', 'cat105', 'cat106', 'cat109', 'cat110', 'cat113', 'cat114', 'cat116']\n",
      "(188318, 130)\n",
      "(125546, 130)\n"
     ]
    }
   ],
   "source": [
    "print('Loadind data from csv files.....')\n",
    "train_data = pd.read_csv('../input/train-dataset/train.csv')\n",
    "test_data  = pd.read_csv('../input/test-orig/test.csv')\n",
    "print('shape of train data :',train_data.shape)\n",
    "print('shape of test data :',test_data.shape)\n",
    "\n",
    "\n",
    "test_data['loss'] = np.nan\n",
    "combined_data = pd.concat([train_data, test_data])\n",
    "print('Joining complete....')\n",
    "print('shape of combined data:',combined_data.shape)\n",
    "\n",
    "\n",
    "categorical_features = list(train_data.select_dtypes(include = ['object']).columns)\n",
    "print('Total number of categorical features in data are :', len(categorical_features))\n",
    "\n",
    "no_common = []\n",
    "for i in categorical_features:\n",
    "    if train_data[i].nunique() != test_data[i].nunique():\n",
    "        no_common.append(i)\n",
    "print('Features that have diff unique values in train and test are :')\n",
    "print(no_common)\n",
    "\n",
    "def log_xgboost_eval_mae(pred,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    a = mean_absolute_error(np.exp(pred),np.exp(labels))\n",
    "    return 'mae',a\n",
    "\n",
    "def search_feature(x):\n",
    "    if x in combined_remaining:\n",
    "        return np.nan\n",
    "    return \n",
    "\n",
    "#Reference https://www.geeksforgeeks.org/python-pandas-factorize/\n",
    "#Reference https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\n",
    "\n",
    "for i in categorical_features:\n",
    "    if train_data[i].nunique() != test_data[i].nunique():\n",
    "        train_unique_set = set(train_data[i].unique())\n",
    "        test_unique_set  = set(test_data[i].unique())\n",
    "        remaining_train  = train_unique_set - test_unique_set\n",
    "        remaining_test   = test_unique_set - train_unique_set\n",
    "        \n",
    "        combined_remaining = remaining_train.union(remaining_test)\n",
    "        \n",
    "        combined_data[i] = combined_data[i].apply(lambda x: search_feature(x),1)\n",
    "    combined_data[i] = pd.factorize(combined_data[i].values,sort = True)[0]\n",
    "    \n",
    "x_train = combined_data[combined_data['loss'].notnull()]\n",
    "x_test = combined_data[combined_data['loss'].isnull()]\n",
    "\n",
    "shift = 200\n",
    "y_train = np.log(x_train['loss']+shift)\n",
    "x_train = x_train.drop(['loss','id'],axis = 1)\n",
    "x_test  = x_test.drop(['loss','id'],axis = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining both x_train and y_train to do the row sampling \n",
    "train = pd.concat([x_train,y_train],axis =1)\n",
    "total_rows = train.shape[0]\n",
    "no_of_rows_in_samples = int(total_rows/7)\n",
    "\n",
    "def row_sampling(train):\n",
    "    sample_df = train.sample(n = no_of_rows_in_samples,replace = True, random_state=1)\n",
    "    return sample_df\n",
    "\n",
    "def split_xy(train):\n",
    "    x = train.drop(['loss'],axis =1)\n",
    "    y = train.loss\n",
    "    return x,y\n",
    "\n",
    "model_1 = XGBRegressor()\n",
    "model_2 = AdaBoostRegressor()\n",
    "model_3 = RandomForestRegressor()\n",
    "model_4 = LinearRegression()\n",
    "model_5 = DecisionTreeRegressor()\n",
    "model_6 = ElasticNet()\n",
    "model_7 = Lasso()\n",
    "model_8 = Ridge()\n",
    "\n",
    "model_list = [model_1,model_2,model_3,model_4,model_5,model_6,model_7,model_8]\n",
    "\n",
    "\n",
    "base_learner_df = pd.DataFrame()\n",
    "base_learner_output = []\n",
    "for i in range(len(model_list)):\n",
    "    train_data1 = row_sampling(train)\n",
    "    x_train1 = train_data1.drop(['loss'],axis =1)\n",
    "    y_train1 = train_data1['loss']\n",
    "    model_list[i].fit(x_train1,y_train1)\n",
    "    y_pred = model_list[i].predict(x_train)\n",
    "    base_learner_output.append(y_pred)\n",
    "    \n",
    "\n",
    "base_learner_df = pd.DataFrame()\n",
    "df_cols = ['xgb','ada','rf','lr','dt','eln','las','rid']\n",
    "for i,j in enumerate(df_cols):\n",
    "    base_learner_df[j] = base_learner_output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining above code.\n",
    "* we concatenated both xtrian and y train cuz we need to do sampling.\n",
    "* row_sampling function is used to sample the data.\n",
    "* we did 8 samples of the data and trained 8 different models for each sample respectively.\n",
    "* used those models to predict whole x train again so that we can use those predictions as input to the meta classifier.\n",
    "* stored all those predictions into bse_learners_df.\n",
    "* we will then split that predictions dataframe into x_train and y_trai again to train meta model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 8)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "x_train2 = base_learner_df\n",
    "y_train2 = train['loss']\n",
    "print(x_train2.shape)\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta classifier as XGBRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "             validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "model.fit(x_train2,y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Test Data Model ready\n",
    "* we pass each row on test data on these above trained base models and predict the output.\n",
    "* we then add all of those predicted output into a dataframe and give that dataframe as a input to meta classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125546, 8)\n"
     ]
    }
   ],
   "source": [
    "list1=[]\n",
    "for i in range(0,8):\n",
    "    list1.append(model_list[0].predict(x_test))\n",
    "    \n",
    "test_df = pd.DataFrame()\n",
    "for i,j in enumerate(df_cols):\n",
    "    test_df[j] = list1[i]\n",
    "\n",
    "df = test_df\n",
    "test_df=(df-df.min())/(df.max()-df.min())\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* Test df contains values which we get from base line models. we pass each test row into different base models and save those prediction values into  df.\n",
    "* and then we pass this df to predict the final output from meta model.\n",
    "* we then use that predictions as final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.exp(model.predict(test_df)) - 200\n",
    "sub = pd.read_csv('../input/submission/sample_submission.csv')\n",
    "sub['loss'] = p_test\n",
    "sub.to_csv('submissions7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* This model performed decent - we got a score of 1240.46498 MAE value and leader board score is 1109.70.\n",
    "* we are way worse than normal models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will try with standardising the data we get from base line models so that we will feed stardardized input to meta model and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 8)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "df = base_learner_df\n",
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "x_train2 = normalized_df\n",
    "y_train2 = train['loss']\n",
    "print(x_train2.shape)\n",
    "print(y_train2.shape)\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train2,y_train2)\n",
    "\n",
    "p_test = np.exp(model.predict(test_df)) - 200\n",
    "sub = pd.read_csv('../input/submission/sample_submission.csv')\n",
    "sub['loss'] = p_test\n",
    "sub.to_csv('submissions8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* This model with standardisation worked really bad - we got a score of 2085.31 and leader board score is 1109.70.\n",
    "* we are way worse than normal models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts:\n",
    "1. why these models worked bad when compared to normal models - one of the main reason is we havent done hyper paramter tuning. Hyper parameter tuning really needed to get best fitting models.\n",
    "1. usually stacking models will work much better than normal models as we are using multiple models to learn more about data.\n",
    "1. we can also use KFold techniques to this process to even get better results. But without hyper parameter tuning even kfold will not yield you to get good results.\n",
    "1. There is a lot of room for improvement here. we got a score of 1114.46 MAE score with good hyper parameter tuned xgboostregressor. so with stacking we can improve our score to like 1112 or 1110 may be, but we need a lot of hyper parameter tuning as we are using so many models.\n",
    "1. Here we used k samples of data to train on k models. we can always train whole dataset on k models and then we use a meta model on top of that. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
