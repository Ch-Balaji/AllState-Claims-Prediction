{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/test-orig/test.csv\n",
      "/kaggle/input/submission/sample_submission.csv\n",
      "/kaggle/input/train-dataset/train.csv\n",
      "/kaggle/input/train-dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind data from csv files.....\n",
      "shape of train data : (188318, 132)\n",
      "shape of test data : (125546, 131)\n",
      "Joining complete....\n",
      "shape of combined data: (313864, 132)\n",
      "Total number of categorical features in data are : 116\n",
      "Features that have diff unique values in train and test are :\n",
      "['cat90', 'cat92', 'cat96', 'cat99', 'cat101', 'cat102', 'cat103', 'cat105', 'cat106', 'cat109', 'cat110', 'cat113', 'cat114', 'cat116']\n",
      "(188318, 130)\n",
      "(125546, 130)\n"
     ]
    }
   ],
   "source": [
    "print('Loadind data from csv files.....')\n",
    "train_data = pd.read_csv('../input/train-dataset/train.csv')\n",
    "test_data  = pd.read_csv('../input/test-orig/test.csv')\n",
    "print('shape of train data :',train_data.shape)\n",
    "print('shape of test data :',test_data.shape)\n",
    "\n",
    "\n",
    "test_data['loss'] = np.nan\n",
    "combined_data = pd.concat([train_data, test_data])\n",
    "print('Joining complete....')\n",
    "print('shape of combined data:',combined_data.shape)\n",
    "\n",
    "\n",
    "categorical_features = list(train_data.select_dtypes(include = ['object']).columns)\n",
    "print('Total number of categorical features in data are :', len(categorical_features))\n",
    "\n",
    "no_common = []\n",
    "for i in categorical_features:\n",
    "    if train_data[i].nunique() != test_data[i].nunique():\n",
    "        no_common.append(i)\n",
    "print('Features that have diff unique values in train and test are :')\n",
    "print(no_common)\n",
    "\n",
    "def log_xgboost_eval_mae(pred,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    a = mean_absolute_error(np.exp(pred),np.exp(labels))\n",
    "    return 'mae',a\n",
    "\n",
    "def search_feature(x):\n",
    "    if x in combined_remaining:\n",
    "        return np.nan\n",
    "    return \n",
    "\n",
    "#Reference https://www.geeksforgeeks.org/python-pandas-factorize/\n",
    "#Reference https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\n",
    "\n",
    "for i in categorical_features:\n",
    "    if train_data[i].nunique() != test_data[i].nunique():\n",
    "        train_unique_set = set(train_data[i].unique())\n",
    "        test_unique_set  = set(test_data[i].unique())\n",
    "        remaining_train  = train_unique_set - test_unique_set\n",
    "        remaining_test   = test_unique_set - train_unique_set\n",
    "        \n",
    "        combined_remaining = remaining_train.union(remaining_test)\n",
    "        \n",
    "        combined_data[i] = combined_data[i].apply(lambda x: search_feature(x),1)\n",
    "    combined_data[i] = pd.factorize(combined_data[i].values,sort = True)[0]\n",
    "    \n",
    "x_train = combined_data[combined_data['loss'].notnull()]\n",
    "x_test = combined_data[combined_data['loss'].isnull()]\n",
    "\n",
    "shift = 200\n",
    "y_train = np.log(x_train['loss']+shift)\n",
    "x_train = x_train.drop(['loss','id'],axis = 1)\n",
    "x_test  = x_test.drop(['loss','id'],axis = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining both x_train and y_train to do the row sampling \n",
    "train = pd.concat([x_train,y_train],axis =1)\n",
    "n_rows = 113008 #60% of data points.\n",
    "\n",
    "def row_sampling(train,seed):\n",
    "    sample_df = train.sample(n = n_rows,replace = False, random_state=seed)\n",
    "    return sample_df\n",
    "\n",
    "def split_xy(train):\n",
    "    x = train.drop(['loss'],axis =1)\n",
    "    y = train.loss\n",
    "    return x,y\n",
    "\n",
    "\n",
    "data1 = row_sampling(train,1)\n",
    "x_train1,y_train1 = split_xy(data1)\n",
    "\n",
    "data2 = row_sampling(train,2)\n",
    "x_train2,y_train2 = split_xy(data2)\n",
    "\n",
    "data3 = row_sampling(train,3)\n",
    "x_train3,y_train3 = split_xy(data3)\n",
    "\n",
    "data4 = row_sampling(train,4)\n",
    "x_train4,y_train4 = split_xy(data4)\n",
    "\n",
    "data5 = row_sampling(train,5)\n",
    "x_train5,y_train5 = split_xy(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestRegressor()\n",
    "model_2 = DecisionTreeRegressor()\n",
    "model_3 = ElasticNet()\n",
    "model_4 = Ridge()\n",
    "model_5 = ExtraTreesRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning for all Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=8, max_features=0.2, max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=2,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False)\n",
      "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=12,\n",
      "                      max_features=0.3, max_leaf_nodes=120,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=15, min_samples_split=10,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best')\n",
      "ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.01,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.01,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
      "                    max_depth=10, max_features=0.3, max_leaf_nodes=None,\n",
      "                    max_samples=None, min_impurity_decrease=0.0,\n",
      "                    min_impurity_split=None, min_samples_leaf=3,\n",
      "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                    n_estimators=150, n_jobs=None, oob_score=False,\n",
      "                    random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [90,100,125,150],\n",
    "    'max_features': [0.2,0.1,0.3],\n",
    "    'max_depth': [7,8,9,10],\n",
    "    'min_samples_leaf': [1,2,3,4],\n",
    "    'min_samples_split': [10, 20, 40]\n",
    "}\n",
    "clf1 = GridSearchCV(estimator = model_1,param_grid = rf_params,cv=3)\n",
    "clf1.fit(x_train1,y_train1)\n",
    "final_model1 = clf1.best_estimator_\n",
    "print(final_model1)\n",
    "\n",
    "dt_params = {\n",
    "      \"min_samples_split\": [10, 20, 40],\n",
    "      \"max_depth\": [2, 6, 8],\n",
    "      \"min_samples_leaf\": [20, 40, 100],\n",
    "      \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "clf2 = GridSearchCV(estimator = model_2,param_grid = dt_params,cv=3)\n",
    "clf2.fit(x_train2,y_train2)\n",
    "final_model2 = clf2.best_estimator_\n",
    "print(final_model2)\n",
    "\n",
    "\n",
    "eln_params = {\n",
    "    'alpha':[1.0,0.1,0.01,0.001,0.005,0.015],\n",
    "    'l1_ratio':[0.4,0.3,0.2,0.1,0.01,0.05]\n",
    "}\n",
    "\n",
    "clf3 = GridSearchCV(estimator = model_3,param_grid = eln_params,cv=3)\n",
    "clf3.fit(x_train3,y_train3)\n",
    "final_model3 = clf3.best_estimator_\n",
    "print(final_model3)\n",
    "\n",
    "rid_params = {\n",
    "    'alpha':[1.0,0.1,0.01,0.001,0.005,0.015]\n",
    "}\n",
    "\n",
    "clf4 = GridSearchCV(estimator = model_4,param_grid = rid_params,cv=3)\n",
    "clf4.fit(x_train4,y_train4)\n",
    "final_model4 = clf3.best_estimator_\n",
    "print(final_model4)\n",
    "\n",
    "et_params = {\n",
    "    'n_estimators': [100,125,150,175],\n",
    "    'max_features': [0.2,0.1,0.3],\n",
    "    'max_depth': [7,8,9,10],\n",
    "    'min_samples_leaf': [1,2,3,4],\n",
    "}\n",
    "clf5 = GridSearchCV(estimator = model_5,param_grid = et_params,cv=3)\n",
    "clf5.fit(x_train5,y_train5)\n",
    "final_model5 = clf5.best_estimator_\n",
    "print(final_model5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting with best hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model1 = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                      max_depth=8, max_features=0.2, max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=2,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                      random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "final_model2 = DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=12,\n",
    "                      max_features=0.3, max_leaf_nodes=120,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=15, min_samples_split=10,\n",
    "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                      random_state=None, splitter='best')\n",
    "\n",
    "final_model4 = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.01,\n",
    "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
    "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\n",
    "final_model3 = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.01,\n",
    "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
    "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\n",
    "\n",
    "final_model5 = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
    "                    max_depth=10, max_features=0.3, max_leaf_nodes=None,\n",
    "                    max_samples=None, min_impurity_decrease=0.0,\n",
    "                    min_impurity_split=None, min_samples_leaf=3,\n",
    "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                    n_estimators=150, n_jobs=None, oob_score=False,\n",
    "                    random_state=None, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                    max_depth=10, max_features=0.3, max_leaf_nodes=None,\n",
       "                    max_samples=None, min_impurity_decrease=0.0,\n",
       "                    min_impurity_split=None, min_samples_leaf=3,\n",
       "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                    n_estimators=150, n_jobs=None, oob_score=False,\n",
       "                    random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model1.fit(x_train1,y_train1)\n",
    "final_model2.fit(x_train2,y_train2)\n",
    "final_model3.fit(x_train3,y_train3)\n",
    "final_model4.fit(x_train4,y_train4)\n",
    "final_model5.fit(x_train5,y_train5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions based on base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "pred1 = final_model1.predict(x_train)\n",
    "pred_list.append(pred1)\n",
    "\n",
    "pred2 = final_model2.predict(x_train)\n",
    "pred_list.append(pred2)\n",
    "\n",
    "pred3 = final_model3.predict(x_train)\n",
    "pred_list.append(pred3)\n",
    "\n",
    "pred4 = final_model4.predict(x_train)\n",
    "pred_list.append(pred4)\n",
    "\n",
    "pred5 = final_model5.predict(x_train)\n",
    "pred_list.append(pred4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting predictions into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learner_df = pd.DataFrame()\n",
    "df_cols = ['RF','DT','ELN','RD','ET']\n",
    "for i,j in enumerate(df_cols):\n",
    "    base_learner_df[j] = pred_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 5)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "x_train_meta = base_learner_df\n",
    "y_train_meta = train['loss']\n",
    "print(x_train_meta.shape)\n",
    "print(y_train_meta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8, eval_metric='mae',\n",
      "             gamma=0, gpu_id=-1, importance_type='gain',\n",
      "             interaction_constraints=None, learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=5, min_child_weight=3, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=0,\n",
      "             num_parallel_tree=3, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=1,\n",
      "             subsample=0.7, tree_method=None, validate_parameters=False,\n",
      "             verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'seed': [0],\n",
    "    'colsample_bytree': [0.7,0.8,0.6],\n",
    "    'silent': [1],\n",
    "    'subsample': [0.5,0.6,0.7],\n",
    "    'learning_rate': [0.01,0.0001,0.1],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [3,4,5],\n",
    "    'num_parallel_tree': [1,2,3],\n",
    "    'min_child_weight': [1,2,3],\n",
    "    'eval_metric': ['mae'],\n",
    "}\n",
    "\n",
    "model_meta = XGBRegressor()\n",
    "clf6 = GridSearchCV(estimator = model_meta,param_grid = xgb_params,cv=3)\n",
    "clf6.fit(x_train_meta,y_train_meta)\n",
    "final_model6 = clf6.best_estimator_\n",
    "print(final_model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, eval_metric='mae',\n",
       "             gamma=0, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints=None, learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=3, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=3, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=1,\n",
       "             subsample=0.7, tree_method=None, validate_parameters=False,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model6.fit(x_train_meta,y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125546, 5)\n"
     ]
    }
   ],
   "source": [
    "list1=[]\n",
    "model_list = [final_model1,final_model2,final_model3,final_model4,final_model5]\n",
    "for i in range(0,5):\n",
    "    list1.append(model_list[i].predict(x_test))\n",
    "    \n",
    "test_df = pd.DataFrame()\n",
    "for i,j in enumerate(df_cols):\n",
    "    test_df[j] = list1[i]\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.exp(final_model6.predict(test_df)) - 200\n",
    "sub = pd.read_csv('../input/submission/sample_submission.csv')\n",
    "sub['loss'] = p_test\n",
    "sub.to_csv('submissions1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* we got a score of 1194.85."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
